openai_news = """SOURCE: https://www.technologyreview.com/2023/11/27/1083886/unpacking-the-hype-around-openais-rumored-new-q-model/
CONTENT:
Unpacking the hype around OpenAI’s rumored new Q* model
If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems.
This story is from The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,
[sign up here](https://forms.technologyreview.com/newsletters/ai-demystified-the-algorithm/).
Ever since last week’s dramatic events at OpenAI, the rumor mill has been in overdrive about why the company’s chief scientific officer, Ilya Sutskever, and its board decided to oust CEO Sam Altman.
While we still don’t know all the details, there have been reports that researchers at OpenAI had made a “breakthrough” in AI that had alarmed staff members.
[Reuters](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/) and [The Information ](https://www.theinformation.com/articles/openai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern?rc=e9y5ks)both report that researchers had come up with a new way to make powerful AI systems and had created a new model, called Q* (pronounced Q star), that was able to perform grade-school-level math. According to the people who spoke to Reuters, some at OpenAI believe this could be a milestone in the company’s quest to build artificial general intelligence, a much-hyped concept referring to an AI system that is smarter than humans. The company declined to comment on Q*.
Social media is full of speculation and excessive hype, so I called some experts to find out how big a deal any breakthrough in math and AI would really be.
Researchers have for years tried to get AI models to solve math problems. Language models like ChatGPT and GPT-4 can do some math, but not very well or reliably. We currently don’t have the algorithms or even the right architectures to be able to solve math problems reliably using AI, says Wenda Li, an AI lecturer at the University of Edinburgh. Deep learning and transformers (a kind of neural network), which is what language models use, are excellent at recognizing patterns, but that alone is likely not enough, Li adds.      
Math is a benchmark for reasoning, Li says. A machine that is able to reason about mathematics, could, in theory, be able to learn to do other tasks that build on existing information, such as writing computer code or drawing conclusions from a news article. Math is a particularly hard challenge because it requires AI models to have the capacity to reason and to really understand what they are dealing with.
A generative AI system that could reliably do math would need to have a really firm grasp on concrete definitions of particular concepts that can get very abstract. A lot of math problems also require some level of planning over multiple steps, says Katie Collins, a PhD researcher at the University of Cambridge, who specializes in math and AI. Indeed, Yann LeCun, chief AI scientist at Meta, posted on X and LinkedIn over the weekend that he thinks Q* is likely to be “OpenAI attempts at planning.”
People who worry about whether AI
[poses an existential risk to humans](https://www.technologyreview.com/2023/06/19/1075140/how-existential-risk-became-biggest-meme-in-ai/), one of OpenAI's founding concerns, fear that such capabilities might lead to rogue AI. Safety concerns might arise if such AI systems are allowed to set their own goals and start to interface with a real physical or digital world in some ways, says Collins.  
But while math capability might take us a step closer to more powerful AI systems, solving these sorts of math problems doesn’t signal the birth of a superintelligence.
“I don’t think it immediately gets us to AGI or scary situations,” says Collins. It’s also very important to underline what kind of math problems AI is solving, she adds.
“Solving elementary-school math problems is very, very different from pushing the boundaries of mathematics at the level of something a Fields medalist can do,” says Collins, referring to a top prize in mathematics.
Machine-learning research has focused on solving elementary-school problems, but
=====

SOURCE: https://www.theguardian.com/commentisfree/2023/nov/27/openai-microsoft-big-tech-monopoly
CONTENT:
The theatrics of OpenAI’s seeming implosion amid the firing of its CEO and co-founder Sam Altman, Microsoft’s dramatic offer to poach its top executives and staff, and Altman’s triumphant return following the ouster of the board has all the trappings of a Hollywood blockbuster.
But the drama unfolding should put the spotlight on the tyranny of the tech titans that control critical aspects of the AI ecosystem.
OpenAI has developed some of the most advanced large-language models and pioneering artificial-intelligence products, such as the text generator ChatGPT and image generator Dall-E, which have been responsible for making generative AI into a household term and discussion about AI risks into dinnertime conversation.
Although OpenAI is in the spotlight, however, Microsoft has played a leading role in the unfolding drama. Microsoft swooped in to scoop up the ousted executives and create a new AI research division for Altman to lead, with hundreds of staff reportedly ready to follow them. Microsoft
[said](https://twitter.com/kevin_scott/status/1726971608706031670) it was ready to hire them all (though they would have probably needed to wait until the new year, when California’s [prohibition against non-competes](https://calemploymentlawupdate.proskauer.com/2023/09/california-expands-prohibition-against-non-competes/) goes into effect) and it has the cash to make good on such a promise.     
It turns out Microsoft won’t have to take on the entire cast of characters, since Altman is now set to return to OpenAI under a new board leadership, which should allow Microsoft to keep its privileged relationship without assuming any liability for employee costs or research and development. Either way, it’s a win-win for Microsoft.
At the root of these theatrics are questions of power: power over the resources needed to develop advanced AI systems and the power to decide how to balance current harms against future risks and shape the future of this technology.
The vast resources needed to develop, train and run cutting-edge AI models reward scale and incentivize companies to seek market dominance, as the Open Market Institute outlined in a
[recent report](https://www.openmarketsinstitute.org/publications/report-ai-in-the-public-interest-confronting-the-monopoly-threat). One way companies do this is by leveraging partnerships, investments and acquisitions to establish control and obtain access.        
OpenAI has received more than $13bn worth of investment since 2019 from Microsoft, which reportedly acquired a 49% stake in the company and the right to three-quarters of OpenAI’s profits. Microsoft also ensured that it would be OpenAI’s
[sole cloud provider](https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/), locking in millions of dollars of value given the computational costs involved in running generative AI products.
While billed as a partnership, the deal looks more like a “killer acquisition” that gives Microsoft unparalleled access to a tech unicorn that was on track for a multibillion-dollar valuation before the shake-up.
This partnership is likely to get even tighter given the new cast of characters brought in to replace the non-profit board that fired Altman,
[reportedly](https://www.nytimes.com/2023/11/21/technology/openai-altman-board-fight.html) over clashing views on how to balance safety and commercialization of the company’s revolutionary AI technology. The new board members appear more aligned with the tech sector’s mantra of “move fast, break things”. They include two members with deep roots in Silicon Valley and Larry Summers, the former treasury secretary with a [track record](https://prospect.org/economy/falling-upward-larry-summers/) of applying “free-market theory where it didn’t fit the circumstances”, in the words of the American Prospect, and cautioning against regulators using anti-trust to address economic concentration.
Microsoft is one of just a handful of gatekeeper firms – namely Alphabet (Google’s parent company), Apple, Amazon and Meta – that have the necessary computing power, access to data, and technical expertise needed to develop advanced AI systems. Their control of the AI development pipeline gives these companies the ability to dictate terms and fees and protect against challengers, as
=====

SOURCE: https://openai.com/blog?topics=announcements
CONTENT:
Close
Search
Submit
Skip to main content
Site Navigation
Research
Overview
Index
GPT-4
DALL·E 3
API
Overview
Data privacy
Pricing
Docs
ChatGPT
Overview
Enterprise
Try ChatGPT
Safety
Company
About
Blog
Careers
Residency
Charter
Security
Customer stories
Search
Navigation quick links
Log in
Try ChatGPT
Menu
Mobile Navigation
Close
Site Navigation
Research
Overview
Index
GPT-4
DALL·E 3
API
Overview
Data privacy
Pricing
Docs
ChatGPT
Overview
Enterprise
Try ChatGPT
Safety
Company
About
Blog
Careers
Residency
Charter
Security
Customer stories
Quick Links
Log in
Try ChatGPT
Search
Submit
Blog
Latest updates
Filter and sort
Filter selections
Topics
Announcements
(69)
Community
(5)
Culture & Careers
(17)
Events
(9)
Product
(30)
Research
(3)
Responsible AI
(4)
Safety & Alignment
(11)
Authors
Scott Aaronson
(1)
Joshua Achiam
(2)
Steven Adler
(1)
Sandhini Agarwal
(2)
Lama Ahmad
(2)
John Allard
(1)
Sam Altman
(6)
Dario Amodei
(1)
Parnian Barekatain
(2)
Mohammad Bavarian
(1)
Gabriel Bernadett-Shapiro
(1)
Greg Brockman
(18)
Jack Clark
(2)
Arka Dhar
(1)
Atty Eleti
(2)
Tyna Eloundou
(3)
Elie Georges
(1)
Vik Goel
(1)
Ian Goodfellow
(2)
Ryan Greene
(1)
Maddie Hall
(1)
Jeff Harris
(1)
Steven Heidel
(1)
Joanne Jang
(3)
Angela Jiang
(2)
Heewoo Jun
(1)
Andrej Karpathy
(1)
Logan Kilpatrick
(3)
Jan Hendrik Kirchner
(1)
Teddy Lee
(1)
Jan Leike
(3)
Jade Leung
(1)
Rachel Lim
(2)
Sam Manning
(1)
Todor Markov
(1)
Luke Miller
(2)
Pamela Mishkin
(1)
Igor Mordatch
(1)
Mira Murati
(1)
Elon Musk
(1)
Arvind Neelakantan
(2)
Harold Nguyen
(1)
Joel Parish
(1)
Andrew Peng
(1)
Ashley Pilipiszyn
(3)
Michelle Pokrass
(1)
Henrique Pondé
(1)
Boris Power
(1)
Bob Rotsted
(1)
Ted Sanders
(1)
Shibani Santurkar
(1)
Girish Sastry
(1)
Larissa Schiavo
(6)
John Schulman
(2)
Ilya Sutskever
(8)
Jie Tang
(2)
Andrea Vallone
(1)
Peter Welinder
(1)
Lilian Weng
(4)
Michael Wu
(2)
Jeffrey Wu
(1)
Wojciech Zaremba
(2)
Chong Zhang
(1)
OpenAI
(61)
Sort options
Sort order
Date: newest
Date: oldest
Current Filters
Announcements
Apply
Reset
Showing 20 of 107 results
Loading…
1 – 20 of 107
Previous page
Next page
Current page:
1
1
2
3
4
5
6
of 6
=====

SOURCE: https://openai.com/blog/openai-announces-leadership-transition
CONTENT:
Chief technology officer Mira Murati appointed interim CEO to lead OpenAI; Sam Altman departs the company.
Search process underway to identify permanent successor.
The board of directors of OpenAI, Inc., the 501(c)(3) that acts as the overall governing body for all OpenAI activities, today announced that Sam Altman will depart as CEO and leave the board of directors. Mira Murati, the company’s chief technology officer, will serve as interim CEO, effective immediately.
A member of OpenAI’s leadership team for five years, Mira has played a critical role in OpenAI’s evolution into a global AI leader. She brings a unique skill set, understanding of the company’s values, operations, and business, and already leads the company’s research, product, and safety functions. Given her long tenure and close engagement with all aspects of the company, including her experience in AI governance and policy, the board believes she is uniquely qualified for the role and anticipates a seamless transition while it conducts a formal search for a permanent CEO.
Mr. Altman’s departure follows a deliberative review process by the board, which concluded that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities. The board no longer has confidence in his ability to continue leading OpenAI.
In a statement, the board of directors said: “OpenAI was deliberately structured to advance our mission: to ensure that artificial general intelligence benefits all humanity. The board remains fully committed to serving this mission. We are grateful for Sam’s many contributions to the founding and growth of OpenAI. At the same time, we believe new leadership is necessary as we move forward. As the leader of the company’s research, product, and safety functions, Mira is exceptionally qualified to step into the role of interim CEO. We have the utmost confidence in her ability to lead OpenAI during this transition period.”
OpenAI’s board of directors consists of OpenAI chief scientist Ilya Sutskever, independent directors Quora CEO Adam D’Angelo, technology entrepreneur Tasha McCauley, and Georgetown Center for Security and Emerging Technology’s Helen Toner.
As a part of this transition, Greg Brockman will be stepping down as chairman of the board and will remain in his role at the company, reporting to the CEO.
OpenAI was founded as a non-profit in 2015 with the core mission of ensuring that artificial general intelligence benefits all of humanity. In 2019, OpenAI restructured to ensure that the company could raise capital in pursuit of this mission, while preserving the nonprofit's mission, governance, and oversight. The majority of the board is independent, and the independent directors do not hold equity in OpenAI. While the company has experienced dramatic growth, it remains the fundamental governance responsibility of the board to advance OpenAI’s mission and preserve the principles of its Charter.
=====

SOURCE: https://www.cnbc.com/2023/11/22/sam-altmans-back-heres-whos-on-the-new-openai-board-and-whos-out.html
CONTENT:
- After several days of crisis and tumult, Sam Altman has returned as the CEO of OpenAI. Three new board members have replaced the previous leadership that ousted Altman.
- OpenAI’s new board doesn’t appear to be fully built. Negotiations are reportedly underway to install representation from Microsoft or other major investors.
- Here’s who’s in, who’s out, and what the changes may mean.
After several days of
[crisis and tumult](https://www.cnbc.com/2023/11/22/some-openai-customers-are-switching-or-thinking-of-switching-to-rivals.html), Sam Altman has [returned as the CEO of OpenAI](https://www.cnbc.com/2023/11/22/openai-brings-sam-altman-back-as-ceo-days-after-ouster.html). Three new board members have replaced the previous leadership that ousted Altman.
OpenAI’s new board doesn’t appear to be fully built. Negotiations are
[reportedly underway](https://www.theinformation.com/articles/microsoft-eyes-seat-on-openais-revamped-board) to install representation from [ Microsoft](/quotes/MSFT/), which has invested billions of dollars in OpenAI, or other major investors.
There’s a notable change in the board’s experience. The previous board included academics and researchers, but OpenAI’s new directors have extensive backgrounds in business and technology.
Microsoft CEO Satya Nadella said in an
[interview with CNBC](https://www.cnbc.com/2023/11/20/microsoft-ceo-nadella-says-openai-governance-needs-to-change-no-matter-where-altman-ends-up.html) on Monday that governance at OpenAI needed to change. Nadella said Wednesday he is “encouraged” by the changes to the company’s board, according to a post on X, formerly known as Twitter.
“We believe this is a first essential step on a path to more stable, well-informed, and effective governance,” he said.
Microsoft, Sequoia Capital, Thrive Capital and Tiger Global are among the OpenAI investors that lack representation on the board but had been pushing to reinstate Altman,
[as CNBC previously reported.](https://www.cnbc.com/2023/11/18/openai-investors-push-to-bring-altman-back-as-ceo-after-fired-by-board.html)
Here’s who’s in, who’s out, and what the changes may mean.
Here are the newest members of OpenAI’s board
Bret Taylor, board chair
Bret Taylor is currently a board member at the e-commerce platform
[Shopify](/quotes/SHOP/). He’s also the former co-CEO of [Salesforce](/quotes/CRM/) and was Twitter’s final board chair prior to Elon Musk’s acquisition of the social media platform.
Taylor co-founded Quip, a collaboration platform that was
[acquired by Salesforce in 2016](https://www.cnbc.com/2016/08/01/salesforcecom-buys-quip-for-582-million.html). That acquisition propelled him into the most senior ranks of the enterprise software company, where he would eventually take the co-CEO title in 2021. Taylor [left Salesforce in January](https://www.cnbc.com/2022/11/30/bret-taylor-steps-down-as-co-ceo-of-salesforce-marc-benioff-stays-on-as-ceo.html).
The executive
[launched his own artificial intelligence venture](https://www.theinformation.com/articles/benchmark-sequoia-back-ex-salesforce-ceos-ai-startup) alongside a former Google executive in February. It isn’t clear whether Taylor’s involvement with his own AI startup will cease with his appointment to lead OpenAI’s board.
Taylor did not immediately respond to CNBC’s request for comment.
Larry Summers
Larry Summers served as Treasury secretary during the Clinton administration and was the president of Harvard University. An economist by training, Summers also led the Obama administration’s National Economic Council during the Global Financial Crisis.
His connections in Washington could be valuable for OpenAI as the company faces continued regulatory scrutiny from lawmakers.        
In December, Summers called OpenAI’s popular generative chatbot ChatGPT a “profound thing for humanity” during
=====

SOURCE: https://www.nytimes.com/2023/11/22/technology/open-ai-board-shakeup.html
CONTENT:
Supported by
[SKIP ADVERTISEMENT](#after-sponsor)
Explaining OpenAI’s Board Shake-Up
Who is off, and who is on? For now there are three members, including one holdover from the board that ousted Sam Altman as C.E.O. last week.
For much of the past year, OpenAI’s board of directors has been criticized as too small and too divided to effectively govern one of the fastest-growing start-ups in Silicon Valley history.
On Friday,
[the board’s dysfunction](https://www.nytimes.com/2023/11/21/technology/openai-altman-board-fight.html) spilled into public view when four of its members fired Sam Altman, OpenAI’s popular and powerful chief executive. The dismissal uncorked five turbulent days, as Mr. Altman rallied almost all of the company’s 770 employees to lobby for the board’s resignation and his reinstatement.
Mr. Altman, 38, returned to the company on Tuesday night, after days of haggling over his job and over the makeup of the board.      
[Subscribe to The Times](https://www.nytimes.com/subscription?campaignId=8WXW7) to read as many articles as you like. [Tripp Mickle](https://www.nytimes.com/by/tripp-mickle) reports on Apple and Silicon Valley for The Times and is based in San Francisco. His focus on Apple includes product launches, manufacturing issues and political challenges. He also writes about trends across the tech industry, including layoffs, generative A.I. and robot taxis. [More about Tripp Mickle](https://www.nytimes.com/by/tripp-mickle) [Mike Isaac](https://www.nytimes.com/by/mike-isaac) is a technology correspondent for The Times based in San Francisco. He regularly covers Facebook and Silicon Valley. [More about Mike Isaac](https://www.nytimes.com/by/mike-isaac) [Karen Weise](https://www.nytimes.com/by/karen-weise) writes about technology and is based in Seattle. Her coverage focuses on Amazon and Microsoft, two of the most powerful companies in America. [More about Karen Weise](https://www.nytimes.com/by/karen-weise) [Cade Metz](https://www.nytimes.com/by/cade-metz) is a technology reporter and the author of “Genius Makers: The Mavericks Who Brought A.I. to Google, Facebook, and The World.” He covers artificial intelligence, driverless cars, robotics, virtual reality and other emerging areas. [More about Cade Metz](https://www.nytimes.com/by/cade-metz) [Kevin Roose](https://www.nytimes.com/by/kevin-roose) is a Times technology columnist and a host of the podcast [“Hard Fork.”](https://www.nytimes.com/column/hard-fork) [More about Kevin Roose](https://www.nytimes.com/by/kevin-roose) [Order Reprints](https://www.parsintl.com/publication/the-new-york-times/)| [Today’s Paper](https://www.nytimes.com/section/todayspaper)| [Subscribe](https://www.nytimes.com/subscriptions/Multiproduct/lp8HYKU.html?campaignId=48JQY)
=====

SOURCE: https://indianexpress.com/article/technology/artificial-intelligence/project-q-star-explained-openai-sam-altman-9041746/     
CONTENT:
Listen to this article
What is Project Q*, the AI breakthrough from OpenAI? 5 reasons why it may threaten humanityx
00:00
The last few days have been topsy-turvy in the tech world, thanks to the high-stakes drama that unfolded at OpenAI, the pioneer in AI technologies of today. It all began with the board consisting of Adam D’Angelo, Tosha McCauley, Ilya Sutskever, and Helen Toner, sacking Sam Altman. A lot of back and forth followed that, with Microsoft offering Altman a job to head a new advanced AI research team.
Meanwhile, nearly 700 employees of the 770 staff from OpenAI wrote an open letter pledging allegiance to Altman and threatened to quit and join
[Microsoft](https://indianexpress.com/about/microsoft/) if the board was not dissolved and Altman was not reinstated. The four-day exile of Altman led to numerous speculations about the cause, from disagreement with board members over products, lack of consistent communication, and differences over AI safety.
Even as all these stories floated around, several staff researchers had reportedly written to the board of directors about the discovery of a powerful AI that could potentially threaten the existence of humanity. Now, more fingers are pointing at this mysterious AI model that could have led to all the drama and chaos at OpenAI. Do note that there is some ambiguity regarding the receipt of this letter, with The Verge reporting that several sources denied its reception by the board.
According to a report in The Information, earlier this year, a team led by OpenAI’s lead scientist Ilya Sutskever made a breakthrough with AI. This later allowed them to build a new model named Q* (read Q-star). It was reported that this new model could solve basic mathematical problems. However, this technological breakthrough also triggered some fears among staff who felt that the AI company did not have enough safeguards in place to ‘commercialise’ such an advanced model.
Q* is essentially an algorithm that is capable of solving elementary mathematical problems by itself, including those that are not part of its training data. This makes it a significant leap towards the much anticipated Artificial General Intelligence (AGI) – a hypothetical capability of AI that makes it perform any intellectual task that the human brain can do. This breakthrough is credited to Sutskever and has been further developed by Szymon Sidor and Jakub Pachoki. The model Q* demonstrates advanced reasoning capabilities similar to humans.
Reportedly, the breakthrough is part of a larger initiative by an AI scientist team that has been formed by combining the Code Gen and Math Gen teams at OpenAI. The team focuses on enhancing the reasoning capabilities of AI models for scientific tasks.
The letter from the researchers reportedly outlined concerns surrounding the system’s potential ability to accelerate scientific progress. At the same time, it also questioned the adequacy of safety measures deployed by OpenAI. According to a report in Reuters, the model reportedly provoked an internal outcry, with staff stating that it could threaten humanity. This warning is believed to be one of the major reasons that led to the sacking of Altman.
Advertisement
Interestingly, Altman had alluded to the making of this model during the interaction at the APEC CEO Summit. He reportedly spoke about the recent technological advance, describing it as something that allowed them to “push the veil of ignorance back and the frontier of discovery forward”. Ever since the OpenAI boardroom saga, this comment by Altman has been seen as him hinting at this breakthrough model.
Advanced logical reasoning and understanding of abstract concepts: All the reports on the internet as of now suggest that Q* has the ability of logical reasoning and understanding abstract concepts. This is a tremendous leap as no AI model so far is capable of it. While on a practical level, it is a breakthrough but this could also lead to unpredictable behaviours or decisions that humans may not be able to foresee or understand beforehand.
Deep learning and programmed rules: Sophia Kalanovska, a researcher told Business Insider that the name Q* implied a fusion
=====

SOURCE: https://www.businessinsider.com/openai-project-q-sam-altman-ia-model-explainer-2023-11
CONTENT:
- A mysterious new OpenAI model known as Q* has got the tech world talking.
- The model is said to have sparked concern at the startup that led to the resulting chaos.
- AI experts say the model could be a big step forward but is unlikely to end the world anytime soon.
As the dust settles on the chaos at OpenAI, we still don't know why CEO Sam Altman was fired — but reports have suggested it could be linked to
[a mysterious AI model](https://www.businessinsider.com/openai-staff-warned-board-of-ai-discovery-before-chaos-report-2023-11).      
The Information reported that a team
[led by OpenAI chief scientist Ilya Sutskever](https://affiliate.insider.com/?h=d5b5c6aa8dca016a58954cebc8efcd14a6b8ddeae76dd7dee1d2d6ffccfa4f79&platform=browser&postID=6560912b49bf0a7df0cdc296&postSlug=openai-project-q-sam-altman-ia-model-explainer-2023-11&site=bi&u=https%3A%2F%2Fwww.theinformation.com%2Farticles%2Fopenai-made-an-ai-breakthrough-before-altman-firing-stoking-excitement-and-concern) had made a breakthrough earlier this year, which allowed them to build a new model known as Q* (pronounced "Q star.") The outlet reported that the model could solve basic math problems.
Sources told Reuters that this model provoked an
[internal firestorm](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/), with several staff members writing a letter to OpenAI's board warning that the new breakthrough could threaten humanity.
This warning was cited as one of the reasons that the board chose to fire Sam Altman, who
[returned as CEO on Wednesday](https://www.businessinsider.com/sam-altman-return-openai-ceo-2023-11) after days of turmoil at the company, Reuters reported.
The ability to solve basic math problems may not sound that impressive, but AI experts told Business Insider it would represent a huge leap forward from existing models, which struggle to generalize outside the data they are trained on.
"If it has the ability to logically reason and reason about abstract concepts, which right now is what it really struggles with, that's a pretty tremendous leap," said Charles Higgins, a cofounder of the AI-training startup Tromero who's also a Ph.D. candidate in AI safety.
He added, "Maths is about symbolically reasoning — saying, for example, 'If X is bigger than Y and Y is bigger than Z, then X is bigger than Z.' Language models traditionally really struggle at that because they don't logically reason, they just have what are effectively intuitions."
Sophia Kalanovska, a fellow Tromero cofounder and Ph.D. candidate, told BI that Q*'s name implied it was a combination of two well-known AI techniques, Q-learning and A* search.
She said this suggested the new model could combine
[the deep-learning techniques that power ChatGPT](https://www.businessinsider.com/how-ai-chatbots-like-chatgpt-work-explainer-2023-7) with rules programmed by humans. It's an approach that could help fix the [chatbot's hallucination problem](https://www.businessinsider.com/chatgpt-generates-error-filled-cancer-treatment-plans-study-2023-8).
"I think it's symbolically very important. On a practical level, I don't think it's going to end the world," Kalanovska said.        
"I think the reason why people believe that Q* is going to lead to AGI is because, from what we've heard so far, it seems like it will combine the two sides of the brain and be capable of knowing some things out of experience, while still being able to reason about facts," she added, referring to artificial general intelligence.
"That is definitely a step closer to what we consider intelligence, and it is possible that it leads to the model being able to have new ideas, which is not
=====

SOURCE: https://www.euronews.com/next/2023/11/24/openai-was-working-on-new-ai-model-that-could-threaten-humanity-before-altmans-ousting-rep
CONTENT:
There are several circulating subvariants of Omicron globally. But what are they and why are we not as concerned as WHO officials about it?
While the height of the pandemic may be over, the virus that causes COVID-19 continues to mutate with multiple variants circulating in every country.
Yet despite this, testing and surveillance have decreased, with experts urging people to keep taking the threat of this disease seriously.
"The world has moved on from COVID, and in many respects, that's good because people are able to stay protected and keep themselves safe, but this virus has not gone anywhere. It's circulating. It's changing, it's killing, and we have to keep up," Maria Van Kerkhove, the COVID-19 technical lead at the World Health Organization (WHO), told Euronews Next.
What are some of the most common COVID variants today?
All the variants circulating today are sublineages of Omicron, a highly transmissible variant of COVID-19 that first emerged two years ago.
One sublineage,
[EG.5, also nicknamed Eris](https://www.euronews.com/next/2023/08/17/eris-what-to-know-about-eg5-the-latest-covid-subvariant), currently represents more than half of the COVID-19 variants circulating globally. It was declared as a variant of interest by WHO back in August.
Cases of EG.5 increased over the summer, but it was recently outpaced in the United States by a closely related subvariant called HV.1. This subvariant now accounts for 29 per cent of the COVID-19 cases in the US, according to the latest figures from the Centres for Disease Control and Prevention (CDC).
"HV.1 is essentially a variant that's derived from EG.5.1 (and previously XBB.1.5) that's just accumulating a few mutations that allow it to better infect people who have immunity to SARS-CoV-2," Andrew Pekosz, a professor of molecular microbiology and immunology at Johns Hopkins University in the US, told Euronews Next.
Pekosz, who studies the replication of respiratory viruses, said that these variants likely emerged as random mutations as part of the natural evolution of viruses.
According to the European Centres for Disease Control and Prevention (ECDC), XBB 1.5-like variants such as EG.5 - or Eris - are currently dominant, making up about 67 per cent of cases in EU/EEA countries.
The prevalence of another Omicron sublineage called BA.2.86 has been "slowly increasing globally," according to WHO, which recently classified it as a "variant of interest". Its sequences were first reported in Israel and Denmark in July and August.
"BA.2.86, when it emerged, was something that was really concerning to scientists because it was a variant that had a large number of mutations, particularly in the spike protein, which is the target for the protective immunity that vaccines and infections give you," said Pekosz.
Scientists think that this variant likely originated in a person with a compromised immune system which allowed the virus to replicate and accumulate mutations at a faster rate, yet it hasn’t come close to becoming dominant.
French authorities, however, recently said that most cases of BA.2.86 in the country were a new sublineage JN.1, which has been "detected in other countries but is mainly circulating in Europe and particularly in France".
It appears to have more mutations that make it more transmissible, Pekosz said.
Should we be concerned about the new variants of COVID?
RNA viruses like SARS-CoV-2, which causes COVID-19, are known to pick up mutations at a faster rate than other viruses "because they make more mistakes and don't have the ability to fix those mistakes," according to Pekosz.
SARS-CoV-2 and its spike protein also appear to tolerate a lot of mutations, similar to what scientists see with influenza.
But so far, while scientists pay attention to these mutations, they are not seeing changes in disease severity, and the tests we use still detect the virus.
These new variants will continue to emerge and "for the most frail in society, especially
====="""