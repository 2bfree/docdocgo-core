# Copy this file to .env and fill in the values

OPENAI_API_KEY="" # your OpenAI API key

# If you are NOT using Azure, the following setting determines the chat model
MODEL_NAME="gpt-3.5-turbo-1106" # another example: "gpt-4-0613"

# If you are using Azure, uncomment the following settings and fill in the values
# OPENAI_API_TYPE="azure" # yours may be different 
# OPENAI_API_BASE="https://techops.openai.azure.com" # yours may be different 
# OPENAI_API_VERSION="2023-05-15" # or whichever version you're using
# CHAT_DEPLOYMENT_NAME="gpt-35-turbo" # your deployment name for the chat model you want to use
# EMBEDDINGS_DEPLOYMENT_NAME="text-embedding-ada-002" # your deployment name for the embedding model

TEMPERATURE="0.1" # 0.0 to 2.0, higher means more random, lower means more conservative (2.0 is jibberish)
LLM_REQUEST_TIMEOUT="3" # seconds to wait before request to the LLM service should be considered timed
# out and the bot should retry sending the request (a few times with exponential back-off).
# For Azure, it seems that a slightly longer timeout is needed, about 9 seconds.

VECTORDB_DIR="./dbs/docdocgo-documentation" # directory of your doc database

## The following item is only relevant if running the API server
DOCDOCGO_API_KEY="" # choose your own 

## The following items are only relevant if docs still need to be ingested
SAVE_VECTORDB_DIR="" # path to directory to save your vector db from ingested docs

# If need to ingest specifically from local docs
DOCS_TO_INGEST_DIR_OR_FILE="" # path to directory or single file with your docs to ingest

## Additional options, used in development (any non-empty string means true)
TWO_BOTS="" # give two different answers instead of one
PRINT_STANDALONE_QUERY="" # print query used for the retriever
PRINT_QA_PROMPT="" # print the prompt used for the final response
PRINT_CONDENSE_QUESTION_PROMPT="" # print the prompt used to condense the question
PRINT_SIMILARITIES="" # print the similarities between the query and the docs